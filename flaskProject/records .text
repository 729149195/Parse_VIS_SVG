各个模块介绍：
    CreateGM: 从原始SVG提取基本数据并做第一步格式处理与部分数据运算（如提取初始bbox及将transfrom应用于提取出的bbox上）
    Add_id: 根据layer为每个元素添加唯一id
    Convert_tiHex: 把原数据的所有色值都转为16进制
    TestGM: 用以测试提取的出bbox边界框是否准确
    Gestalt_Edges_Features: 从基本数据中提取两可视节点间的格式塔特征矩阵
    Neural_Network_Weight: 通过深度神经网络学习到三层权重
    Community_Detection: 用三层权重实现多重社区检测
    Quantification_Basis: 根据各个社区的内部结构和外部结构来推断其被人感知到的顺序
    Statitication: 用以将所有所需数据调整为前端D3可直接调用的数据结构


各个json文件的用途：
GMinfo.json: CreateGM从原始数据中提取并初步处理的元素网络
extracted_nodes.json: 从初步网络中提取并二次处理出的可视节点中提取出的节点基本信息
similarity_graph.json: 根据二次提取出的节点基本信息来张开节点之间的格式塔特征矩阵
community_data.json: 用于存储社区检测结果，用于返回前端进行社区节点凸包渲染
attr_num.json: 用以统计节点的属性数据（未完成）
ele_num.json: 用以统计元素的种类和数量
group_data: 用以展示每个社区的内链接强度和外连接强度（未完成）


BBOX的格式：
    rect: bbox = [[x, y], [x + width, y + height], [x + width / 2, y + height / 2]]
    circle: bbox = [[cx - r, cy - r], [cx + r, cy + r], [cx, cy]]
    line: bbox = [[x1, y1], [x2, y2], [(x1 + x2) / 2, (y1 + y2) / 2]]
    ellipse: bbox = [[cx - rx, cy - ry], [cx + rx, cy + ry], [cx, cy]]
    polygon\polyline: bbox = [[x1, y1], [x2, y2]...points中的所有点]
    text\image: bbox = [[x, y], [x + width, y + height], [x + width/2, y + height/2]]
    path: 通过Pcode和Pnum来拟合成line集， 如[line1, line2, line3....]


格式塔特征矩阵：
{
  "color_similarity": {
    "fill_color_difference": 0.0, // 范围[0, 1]，使用CIEDE2000或其他色差算法计算    //色调！ hue
    "opacity_difference": 0.0, // 范围[0, 1]，直接计算Alpha值差异的相对值
    "brightness_difference": 0.0, // 范围[0, 1]，计算HSL/HSV的L或V值差异
    "saturation_difference": 0.0 // 范围[0, 1]，计算HSL/HSV的S值差异
    "stroke_color_similarity": 0.0 // 边框色相似度，范围[0, 1],使用CIEDE2000或其他色差算法计算, 使用CIEDE2000或其他色差算法计算  //理由
    "stroke_width_similarity": 0.0, // 线宽相似度，范围[0, 1]
    //线型
  },
  "tag_similarity": {
    "tag_match": 0, // 值为1或0，表示是否完全匹配  //
    "layer_similarity": 0.0, // 范围[0, 1]，基于共同父级数量与总深度的比例
    "text_content_similarity": 0.0 // 范围[0, 1]，使用文本相似度算法，空文本为0    //改成字号，放一放
  },
  "position_similarity": {
    "top_edge_similarity": 0.0, // 范围[0, 1]，基于bbox上边界的差异
    "bottom_edge_similarity": 0.0, // 范围[0, 1]，基于bbox下边界的差异
    "left_edge_similarity": 0.0, // 范围[0, 1]，基于bbox左边界的差异
    "right_edge_similarity": 0.0, // 范围[0, 1]，基于bbox右边界的差异
    //中点
    "area_similarity": 0.0, // 范围[0, 1]，基于节点面积的比例差异
    //长宽
    "overlap_ratio": 0.0 // 范围[0, 1]，两个节点的bbox交集与并集的比例
  }
}
最后按顺序生成一列特征数组

1 2 3

1、2、3、123 //


"svg/svg/g/path",
"svg/svg/g/path_1",
[
    1.0,    //color_similarity
    1.0,
    0.0,
    0.0,
    1.0,
    0.8,

    0.5,    //tag_similarity
    0.667,
    0.0,

    0.75,   //position_similarity
    1,
    1,
    0,
    1.0,
    0.333
]


再计算weight之前，先进行权重的预处理
为避免过多相近节点重复渲染，当两个节点的特征矩阵十分接近的时候，则另一个节点不再参与后续的相似矩阵计算，并被加入到待删集，从community_data.json删去（类似取平均）


根据不同的weight进行不同的社区检查
打算先设定为三组，一组社区将color_similarity、tag_similarity、position_similarity都考虑进去，第二组分别考虑其中的两组，第三组分别考虑三种similarity中的单独一种
最后生成

对于社区检测中一个节点处于不同group的考虑：
①所有节点都有一个初始的尽量细致的分组，然后后续的分组都是初始分组的排列组合，即每个节点都只有一个group编号(如果初始分组不细致，后序的排列组合也不准确，好处是处理起来比较方便):
     {
        "id": "svg/svg/g/path",
        "group": 1
     },
     {
        "id": "svg/svg/g/path_1",
        "group": 2
     }
    var 1_groups = [[0,1,2,3,4],[3,4,5],[4,6,7]];
    var 2_groups = [[0,1,2,3,4],[3,4,5],[4,6,7]];
    var 3_groups = [[0,1,2,3,4],[3,4,5],[4,6,7]];
    var 123_groups = [[0,1],[5],[7]];

②所有节点都附带5个group编号，表示其所在的不同group，这5个group可重复(分得比较细致，但资源消耗比较大)：
     {
        "id": "svg/svg/g/path",
        "1_group": 1,
        "2_group": 2,
        "3_group": 1,
        "123_group": 0
     },
     {
        "id": "svg/svg/g/path_1",
        "1_group": 1,
        "2_group": 2,
        "3_group": 1,
        "123_group": 6
     }
去掉一些特征、把过滤的信息变成拖拉的条
//rect识别
图例写全，箭头变为到轴的末尾


训练和测试用数据集：
①用以训练分组权重神经网络模型（自己标注数据）：//四选1
由于只是分组，所以可以自己来标注多份
选择以上①或②的分组方法进行手动标注，或者看看能不能用现有的AI帮助标注
问卷：左彩右灰    总结规律


特异性
显眼性
找模型
图像分割的算法
层次聚类

②用以预测社区内外链接强度对注意力的影响（收集用户数据）：
涉及到人的注意力感知，需要不同用户的数据
以获得社区的内连接强度和外连接强度，作为先验条件，通过贝叶斯预测其被感知到的可能性

GPU: NVIDIA GeForce MX250
NVCUDA64.DLL 31.0.15.4633 NVIDIA CUDA 12.3.107 driver
conda install pytorch torchvision cudatoolkit=12.1




我有一个节点和边的数据：
{
    "Gestalt_Edges": [
        [
            "svg/svg/g/path",
            "svg/svg/g/path_1",
            [
                1.0,
                1.0,
                0.0,
                0.0,
                1.0,
                0.8,
                1.0,
                0.667,
                1.0,
                0.167,
                1.0,
                1.0,
                0.0,
                1.0,
                0.333
            ]
        ],
        [
            "svg/svg/g/path",
            "svg/svg/g_1/polygon",
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.423,
                0.8,
                0.5,
                0.333,
                1.0,
                0.167,
                0.667,
                0.0,
                0.0,
                0.36,
                0.0
            ]
        ],
        [
            "svg/svg/g/path",
            "svg/svg/g_1/polyline",
            [
                0.0,
                1.0,
                0.0,
                0.0,
                0.423,
                0.8,
                0.5,
                0.333,
                1.0,
                0.167,
                0.667,
                0.0,
                0.0,
                0.36,
                0.0
            ]
        ]]
}
其中的数组中的数值分别表示：
格式塔特征矩阵：
{
  "color_similarity": {
    "fill_color_difference": 0.0, // 范围[0, 1]，使用CIEDE2000或其他色差算法计算    //色调！ hue
    "opacity_difference": 0.0, // 范围[0, 1]，直接计算Alpha值差异的相对值
    "brightness_difference": 0.0, // 范围[0, 1]，计算HSL/HSV的L或V值差异
    "saturation_difference": 0.0 // 范围[0, 1]，计算HSL/HSV的S值差异
    "stroke_color_similarity": 0.0 // 边框色相似度，范围[0, 1],使用CIEDE2000或其他色差算法计算, 使用CIEDE2000或其他色差算法计算  //理由
    "stroke_width_similarity": 0.0, // 线宽相似度，范围[0, 1]
    //线型
  },
  "tag_similarity": {
    "tag_match": 0, // 值为1或0，表示是否完全匹配  //
    "layer_similarity": 0.0, // 范围[0, 1]，基于共同父级数量与总深度的比例
    "text_content_similarity": 0.0 // 范围[0, 1]，使用文本相似度算法，空文本为0    //改成字号，放一放
  },
  "position_similarity": {
    "top_edge_similarity": 0.0, // 范围[0, 1]，基于bbox上边界的差异
    "bottom_edge_similarity": 0.0, // 范围[0, 1]，基于bbox下边界的差异
    "left_edge_similarity": 0.0, // 范围[0, 1]，基于bbox左边界的差异
    "right_edge_similarity": 0.0, // 范围[0, 1]，基于bbox右边界的差异
    //中点
    "area_similarity": 0.0, // 范围[0, 1]，基于节点面积的比例差异
    //长宽
    "overlap_ratio": 0.0 // 范围[0, 1]，两个节点的bbox交集与并集的比例
  }
}
我希望使用这个Contrastive-Clustering为其每条边根据这15个similarity计算一个总的相似性权重可以吗。


